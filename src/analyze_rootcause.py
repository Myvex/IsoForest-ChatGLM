#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å¤šè½®æ ¹å› åˆ†æè„šæœ¬
ä½œè€…ï¼šChatGLM+OpenAI
è¯´æ˜ï¼š
    1. è¯»å–æ—¥å¿— CSVï¼Œç­›é€‰ anomaly != -1 çš„è¡Œ
    2. åŠ è½½ ChatGLM-6Bï¼ˆå¯æ”¹ä¸º 1B/2Bï¼‰
    3. å¯¹æ¯æ¡å¼‚å¸¸åšä¸€æ¬¡å¤šè½®åˆ†æ
    4. å¯é€‰è¿›å…¥äº¤äº’æ¨¡å¼ç»§ç»­è¿½é—®
    5. ç»“æœå†™å…¥ output CSV
ä½¿ç”¨æ–¹å¼ï¼š
    python analyze_rootcause.py --input logs.csv --output report.csv
    python analyze_rootcause.py --input logs.csv --output report.csv --interactive
"""

import argparse
import os
import sys
import time
from typing import List, Dict, Tuple

import pandas as pd
from tqdm import tqdm

# ---------- ä¾èµ–æ£€æŸ¥ ----------
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
except ImportError:
    print("è¯·å…ˆå®‰è£… transformersï¼špip install transformers sentencepiece tqdm")
    sys.exit(1)


# ---------- 1. è¯»å– & è¿‡æ»¤ ----------
def load_anomaly_logs(csv_path: str) -> pd.DataFrame:
    """è¯»å– CSV å¹¶ç­›é€‰å¼‚å¸¸è¡Œï¼ˆanomaly != -1ï¼‰"""
    df = pd.read_csv(csv_path)
    if 'anomaly' not in df.columns:
        raise ValueError("CSV å¿…é¡»åŒ…å« 'anomaly' åˆ—")
    return df[df['anomaly'] != -1].reset_index(drop=True)


# ---------- 2. ChatGLM åŠ è½½ ----------
def load_chatglm(model_name: str = "THUDM/chatglm-6b") -> Tuple[AutoTokenizer, AutoModelForCausalLM, pipeline]:
    """è¿”å› tokenizerã€modelã€pipeline"""
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} ...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, trust_remote_code=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name, device_map="auto", trust_remote_code=True
    )
    chat_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
    )
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return tokenizer, model, chat_pipe


# ---------- 3. ç”Ÿæˆå¤šè½®å¯¹è¯ ----------
def ask_chatglm(
    pipe: pipeline,
    prompt: str,
    history: List[Dict[str, str]] = None,
) -> Tuple[str, List[Dict[str, str]]]:
    """
    pipe: pipeline å¯¹è±¡
    prompt: ç”¨æˆ·è¾“å…¥
    history: ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨ [{'role': 'user'/'assistant', 'content': str}]
    è¿”å›ï¼šå›ç­”å­—ç¬¦ä¸²ã€æ›´æ–°åçš„ history
    """
    if history is None:
        history = []

    # æ‹¼æ¥æˆè¿è´¯çš„ä¸Šä¸‹æ–‡
    full_prompt = ""
    for turn in history:
        full_prompt += f"{turn['role']}: {turn['content']}\n"
    full_prompt += f"user: {prompt}\nassistant:"

    # è°ƒç”¨æ¨¡å‹
    result = pipe(full_prompt, do_sample=True, temperature=0.7, top_p=0.9, max_new_tokens=512)
    raw_text = result[0]["generated_text"].strip()

    # åªå– assistant ä¹‹åçš„å†…å®¹
    reply = raw_text.split("assistant:")[-1].strip()

    # æ›´æ–°å†å²
    history.append({"role": "user", "content": prompt})
    history.append({"role": "assistant", "content": reply})

    return reply, history


# ---------- 4. å•æ¡å¼‚å¸¸åˆ†æ ----------
def analyze_single(
    pipe: pipeline,
    row: pd.Series,
    verbose: bool = False,
) -> Tuple[str, List[Dict[str, str]]]:
    """
    å¯¹å•æ¡å¼‚å¸¸åšä¸€æ¬¡å¤šè½®åˆ†æ
    è¿”å›ï¼šåˆ†æç»“æœå­—ç¬¦ä¸²ã€å¯¹è¯å†å²
    """
    # ç¬¬ 1 è½®ï¼šç»™å‡ºæ ¹å› ä¸æ’æŸ¥æ€è·¯
    prompt1 = (
        f"ä»¥ä¸‹æ˜¯ä¸€æ¡ç³»ç»Ÿæ—¥å¿—ï¼Œå‡ºç°äº†å¼‚å¸¸ï¼š\n"
        f"timestamp: {row['timestamp']}\n"
        f"message: {row['message']}\n"
        f"è¯·å…ˆç»™å‡ºè¿™æ¡å¼‚å¸¸çš„å¯èƒ½æ ¹å› ï¼ˆæŠ€æœ¯å±‚é¢ï¼‰ä»¥åŠæ’æŸ¥æ€è·¯ã€‚"
    )
    reply1, history = ask_chatglm(pipe, prompt1)

    if verbose:
        print("\n--- ç¬¬ 1 è½®å›ç­” ---")
        print(reply1)

    # ç¬¬ 2 è½®ï¼šè¿›ä¸€æ­¥è¿½é—®ï¼ˆå¯è‡ªè¡Œæ‰©å±•ï¼‰
    prompt2 = (
        "ä½ æåˆ°å¯èƒ½æ˜¯ç¡¬ä»¶æˆ–ç½‘ç»œå¯¼è‡´çš„è¶…æ—¶ï¼Œè¯·é—®åœ¨æ’æŸ¥æ—¶éœ€è¦æ£€æŸ¥å“ªäº›ç¡¬ä»¶æŒ‡æ ‡ï¼Ÿ"
    )
    reply2, history = ask_chatglm(pipe, prompt2, history)

    if verbose:
        print("\n--- ç¬¬ 2 è½®å›ç­” ---")
        print(reply2)

    # ç¬¬ 3 è½®ï¼šè½¯ä»¶å±‚é¢
    prompt3 = (
        "å¦‚æœç¡¬ä»¶ OKï¼Œæ˜¯å¦è¿˜æœ‰è½¯ä»¶å±‚é¢çš„åŸå› ï¼Ÿæ¯”å¦‚å›ºä»¶ç‰ˆæœ¬ã€é©±åŠ¨ã€é…ç½®ï¼Ÿ"
    )
    reply3, history = ask_chatglm(pipe, prompt3, history)

    if verbose:
        print("\n--- ç¬¬ 3 è½®å›ç­” ---")
        print(reply3)

    # åˆå¹¶æ‰€æœ‰å›ç­”
    full_analysis = "\n\n".join([reply1, reply2, reply3])
    return full_analysis, history


# ---------- 5. ä¸»æµç¨‹ ----------
def main(args):
    # è¯»å–æ—¥å¿—
    df = load_anomaly_logs(args.input)
    print(f"ğŸ“Š å…± {len(df)} æ¡å¼‚å¸¸å¾…åˆ†æ")

    # åŠ è½½æ¨¡å‹
    _, _, chat_pipe = load_chatglm(args.model)

    # ç»“æœåˆ—è¡¨
    results = []

    # é€æ¡åˆ†æ
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="åˆ†æè¿›åº¦"):
        analysis, _ = analyze_single(chat_pipe, row, verbose=args.verbose)
        results.append({
            "timestamp": row["timestamp"],
            "message": row["message"],
            "analysis": analysis,
        })

    # å†™å› CSV
    out_df = pd.DataFrame(results)
    out_df.to_csv(args.output, index=False, encoding="utf-8-sig")
    print(f"\nâœ… åˆ†æå®Œæˆï¼Œç»“æœå·²å†™å…¥ {args.output}")

    # äº¤äº’æ¨¡å¼
    if args.interactive:
        print("\nğŸ”„ è¿›å…¥äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥ q é€€å‡ºï¼‰")
        history = []
        while True:
            user_input = input("\nä½ : ").strip()
            if user_input.lower() in {"q", "quit", "exit"}:
                print("é€€å‡ºäº¤äº’æ¨¡å¼")
                break
            reply, history = ask_chatglm(chat_pipe, user_input, history)
            print(f"ChatGLM: {reply}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="å¤šè½®æ ¹å› åˆ†æè„šæœ¬ï¼ˆChatGLMï¼‰"
    )
    parser.add_argument(
        "--input",
        required=True,
        help="è¾“å…¥æ—¥å¿— CSV è·¯å¾„ï¼ˆåŒ…å« timestamp, message, anomaly åˆ—ï¼‰",
    )
    parser.add_argument(
        "--output",
        default="analysis_report.csv",
        help="è¾“å‡ºåˆ†æç»“æœ CSV è·¯å¾„",
    )
    parser.add_argument(
        "--model",
        default="THUDM/chatglm-6b",
        help="ChatGLM æ¨¡å‹åç§°ï¼ˆå¯æ¢æˆ 1B/2B ç­‰ï¼‰",
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="åˆ†æå®Œåè¿›å…¥äº¤äº’æ¨¡å¼ç»§ç»­è¿½é—®",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="åœ¨åˆ†æè¿‡ç¨‹ä¸­æ‰“å°æ¯è½®å›ç­”",
    )
    args = parser.parse_args()
    main(args)
